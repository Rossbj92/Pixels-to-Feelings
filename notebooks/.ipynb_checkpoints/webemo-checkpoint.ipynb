{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(sys.path[0].replace('notebooks', 'src'))\n",
    "\n",
    "import modeling.modeling_utils as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.torch.cuda.is_available() # Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "device = m.torch.device(\"cuda\" if m.torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "m.torch.backends.cudnn.benchmark=True # Helps optimize training w/ GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The WEBEmo dataset is fairly massive and unable to be uploaded. The ```image-gather``` notebook can be used to download all images, and this notebook contains the code to train a model using curriculum learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train, test = m.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file</th>\n",
       "      <th>lvl_three</th>\n",
       "      <th>lvl_one</th>\n",
       "      <th>lvl_two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_220_F_83683073_O4yJOnarzTjKXuUBAgkAifmiC8d0I...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20_220_F_5292725_818KTy3xv82nEkNolcs2m37MOV86s...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20_220_F_47187567_lwYwc9UQtBK5Be6v4P7HNsCc4Hhr...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1_220_F_38932828_Osns7NBWCq8AhJonYpQArrToDLLhT...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1_220_F_97168737_y0VWy7kLMby9BO6lHDfpyfNpW9o0S...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               file  lvl_three  \\\n",
       "0           0  1_220_F_83683073_O4yJOnarzTjKXuUBAgkAifmiC8d0I...          1   \n",
       "1           1  20_220_F_5292725_818KTy3xv82nEkNolcs2m37MOV86s...         20   \n",
       "2           2  20_220_F_47187567_lwYwc9UQtBK5Be6v4P7HNsCc4Hhr...         20   \n",
       "3           3  1_220_F_38932828_Osns7NBWCq8AhJonYpQArrToDLLhT...          1   \n",
       "4           4  1_220_F_97168737_y0VWy7kLMby9BO6lHDfpyfNpW9o0S...          1   \n",
       "\n",
       "   lvl_one  lvl_two  \n",
       "0        0        0  \n",
       "1        1        1  \n",
       "2        1        1  \n",
       "3        0        0  \n",
       "4        0        0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br>\n",
    "For training, I split the data into 90/10 train/validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_split, val_split = m.train_val_split(X=train['file'], \n",
    "                                           y=train['lvl_one'], \n",
    "                                           test_size=.1, \n",
    "                                           random_state=713)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Pytorch has some pretty neat classes to help load in data. First, I define the transforms that will be used as photos are iteratively loaded during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_transforms = m.transforms.Compose([m.transforms.Resize(256),\n",
    "                                         m.transforms.RandomCrop(224),\n",
    "                                         m.transforms.ToTensor(),\n",
    "                                         m.transforms.Normalize(mean=[0.485, 0.456, 0.406], #OG means/sds from imagenet\n",
    "                                                                std=[0.229, 0.224, 0.225])\n",
    "                                        ])\n",
    "val_transforms = m.transforms.Compose([m.transforms.Resize(256),\n",
    "                                       m.transforms.CenterCrop(224),\n",
    "                                       m.transforms.ToTensor(),\n",
    "                                       m.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                              std=[0.229, 0.224, 0.225])\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Put data into Pytorch Dataset class\n",
    "l1_train_dataset = m.ImgDataset(df=train_split,\n",
    "                                root_dir='../data/images/train',\n",
    "                                percent_sample=1,\n",
    "                                transform=train_transforms)\n",
    "l1_val_dataset = m.ImgDataset(df=val_split,\n",
    "                              root_dir='../data/images/train',\n",
    "                              percent_sample=1,\n",
    "                              transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Pytorch DataLoader iteratively loads minibatches during training\n",
    "l1_loaders = {'train': m.DataLoader(l1_train_dataset,\n",
    "                               batch_size=128, \n",
    "                               shuffle=True,\n",
    "                               #pin_memory=True, # Only use pin_memory with GPU\n",
    "                               num_workers=4), \n",
    "              'val': m.DataLoader(l1_val_dataset, \n",
    "                             batch_size=128,\n",
    "                             #pin_memory=True,\n",
    "                             num_workers=4)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "I used the ```resnet50``` model from Pytorch as the base for the first level. While the pretrained weights were loaded to initialize training, I didn't freeze any layers. That is, I fine-tuned the ```resnet50``` model instead of using it purely for feature extraction. \n",
    "\n",
    "The ```resnet50``` fully-connected classifier layer is of the form ```(input-features, output classes)```, with the original ```output classes``` being 1000 for imagenet; this is simply changed to 2 for the first level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Get everything set up\n",
    "l1_model = m.models.resnet50(pretrained=True)\n",
    "num_ftrs = l1_model.fc.in_features\n",
    "l1_model.fc = m.nn.Linear(num_ftrs, 2) \n",
    "l1_model = l1_model.to(device)\n",
    "l1_criterion = m.nn.CrossEntropyLoss().to(device)\n",
    "l1_optim = m.torch.optim.SGD(l1_model.parameters(), \n",
    "                                   lr=0.01, \n",
    "                                   momentum=0.9, \n",
    "                                   weight_decay=0.0001)\n",
    "# Lower learning rate after 5 epochs of no validation loss\n",
    "l1_scheduler = lr_scheduler.ReduceLROnPlateau(optim_l1_model, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "l1_model_train = train_model(model=l1_model, \n",
    "                             dataloader=l1_loaders, \n",
    "                             criterion=l1_criterion, \n",
    "                             optimizer=l1_optim,\n",
    "                             save_path='../models/l1model.tar',\n",
    "                             num_epochs=50,\n",
    "                             scheduler=l1_scheduler,\n",
    "                             early_stopping=m.EarlyStopping(patience=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Level 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The idea behind curriculum learning is to sequentially expose the model to more complex discriminative tasks in increasing difficulty. Level 2 contains the exact same images, but now, there are 6 classes to predict. Below, the level 1 model is initialized, and only 2 things are different from the level 1 training:\n",
    "1. The learning rate is 1/10 that of level 1 (i.e., 0.001 instead of 0.01)\n",
    "2. The fully-connected layers are modified to classify level 2 labels (i.e., 6 instead of 2 possibilities)\n",
    "<br>\n",
    "\n",
    "First, I make new dataset/dataloader classes for the level-2 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "l2_train_split = train_split.merge(train, left_on = 'file', right_on = 'file')[['file', 'lvl_two']]\n",
    "l2_val_split = val_split.merge(train, left_on = 'file', right_on = 'file')[['file', 'lvl_two']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "l2_train_dataset = m.ImgDataset(df=l2_train_split,\n",
    "                                root_dir='../data/images/train',\n",
    "                                percent_sample=1,\n",
    "                                transform=train_transforms)\n",
    "l2_val_dataset = m.ImgDataset(df=l2_val_split,\n",
    "                              root_dir='../data/images/train',\n",
    "                              percent_sample=1,\n",
    "                              transform=val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The level-2 and level-3 data also suffer from class imbalance. To deal with that during training, I use Pytorch's ```WeightedRandomSampler```. By assigning weights to each class, the minibatches become approximately evenly distributed among the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "l2_samples_weights = m.weighted_sample(l2_train_split, 'lvl_two')\n",
    "l2_weighted_sampler = m.WeightedRandomSampler(weights=l2_samples_weights, num_samples=len(l2_samples_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "l2_loaders = {'train': m.DataLoader(l2_train_dataset,\n",
    "                                    batch_size=128, \n",
    "                                    sampler=l2_weighted_sampler,\n",
    "                                    #pin_memory=True, \n",
    "                                    num_workers=4),\n",
    "              'val': m.DataLoader(l2_val_dataset, \n",
    "                             batch_size=128,\n",
    "                             #pin_memory=True,\n",
    "                             num_workers=4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Initialize model \n",
    "l2_model = m.load_model(path='../models/l1model.tar', \n",
    "                        base=m.models.resnet50(pretrained=False), \n",
    "                        old_classes=2, \n",
    "                        new_classes=6, \n",
    "                        device=device)\n",
    "l2_criterion = m.nn.CrossEntropyLoss().to(device)\n",
    "l2_optim = m.torch.optim.SGD(l2_model.parameters(), \n",
    "                                   lr=0.001, \n",
    "                                   momentum=0.9, \n",
    "                                   weight_decay=0.0001)\n",
    "l2_scheduler = lr_scheduler.ReduceLROnPlateau(l2_optim, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "l2_model_train = train_model(model=l2_model, \n",
    "                             dataloader=l2_loaders, \n",
    "                             criterion=l2_criterion, \n",
    "                             optimizer=l2_optim,\n",
    "                             save_path='../models/l2model.tar',\n",
    "                             num_epochs=50,\n",
    "                             scheduler=l2_scheduler,\n",
    "                             early_stopping=m.EarlyStopping(patience=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Level 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The procedural modifications from level 2 to level 3 are minimal. I still make new dataset/dataloader classes, a weighted sampler, and initialize from the optimal model 2. There are only 2 major changes:\n",
    "1. The learning rate is lowered to 0.0001\n",
    "2. The fully-connected layer now outputs 25 instead of 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "l3_train_split = train_split.merge(train, left_on = 'file', right_on = 'file')[['file', 'lvl_three']]\n",
    "l3_val_split = val_split.merge(train, left_on = 'file', right_on = 'file')[['file', 'lvl_three']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "l3_train_dataset = m.ImgDataset(df=l3_train_split,\n",
    "                                root_dir='../data/images/train',\n",
    "                                percent_sample=1,\n",
    "                                transform=train_transforms)\n",
    "l3_val_dataset = m.ImgDataset(df=l3_val_split,\n",
    "                              root_dir='../data/images/train',\n",
    "                              percent_sample=1,\n",
    "                              transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "l3_samples_weights = m.weighted_sample(l3_train_split, 'lvl_three')\n",
    "l3_weighted_sampler = m.WeightedRandomSampler(weights=l3_samples_weights, num_samples=len(l3_samples_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "l3_loaders = {'train': m.DataLoader(l3_train_dataset,\n",
    "                               batch_size=128, \n",
    "                               sampler=l3_weighted_sampler,\n",
    "                               #pin_memory=True,\n",
    "                               num_workers=4),\n",
    "              'val': m.DataLoader(l3_val_dataset, \n",
    "                             batch_size=128,\n",
    "                             #pin_memory=True,\n",
    "                             num_workers=4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "l3_model = m.load_model(path='../models/l2model.tar', \n",
    "                        base=m.models.resnet50(pretrained=False), \n",
    "                        old_classes=6, \n",
    "                        new_classes=25, \n",
    "                        device=device)\n",
    "l3_criterion = m.nn.CrossEntropyLoss().to(device)\n",
    "l3_optim = m.torch.optim.SGD(l3_model.parameters(), \n",
    "                                   lr=0.0001, \n",
    "                                   momentum=0.9, \n",
    "                                   weight_decay=0.0001)\n",
    "l3_scheduler = lr_scheduler.ReduceLROnPlateau(l3_optim, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "l3_model_train = train_model(model=l3_model, \n",
    "                             dataloader=l3_loaders, \n",
    "                             criterion=l3_criterion, \n",
    "                             optimizer=l3_optim,\n",
    "                             save_path='../models/l3model.tar',\n",
    "                             num_epochs=50,\n",
    "                             scheduler=l3_scheduler,\n",
    "                             early_stopping=m.EarlyStopping(patience=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
